{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d468e9a",
   "metadata": {},
   "source": [
    "## Transformer - Attention is all you need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dcf681",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "import math\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "torch.manual_seed(23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2cbd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6623a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3103d45f",
   "metadata": {
    "code_folding": [
     30,
     94
    ]
   },
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len = MAX_SEQ_LEN):\n",
    "        super().__init__()\n",
    "        self.pos_embed_matrix = torch.zeros(max_seq_len, d_model, device=device)\n",
    "        token_pos = torch.arange(0, max_seq_len, dtype = torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() \n",
    "                             * (-math.log(10000.0)/d_model))\n",
    "        self.pos_embed_matrix[:, 0::2] = torch.sin(token_pos * div_term)\n",
    "        self.pos_embed_matrix[:, 1::2] = torch.cos(token_pos * div_term)\n",
    "        self.pos_embed_matrix = self.pos_embed_matrix.unsqueeze(0).transpose(0,1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "#         print(self.pos_embed_matrix.shape)\n",
    "#         print(x.shape)\n",
    "        return x + self.pos_embed_matrix[:x.size(0), :]\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model = 512, num_heads = 8):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, 'Embedding size not compatible with num heads'\n",
    "        \n",
    "        self.d_v = d_model // num_heads\n",
    "        self.d_k = self.d_v\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask = None):\n",
    "        batch_size = Q.size(0)\n",
    "        '''\n",
    "        Q, K, V -> [batch_size, seq_len, num_heads*d_k]\n",
    "        after transpose Q, K, V -> [batch_size, num_heads, seq_len, d_k]\n",
    "        '''\n",
    "        Q = self.W_q(Q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2 )\n",
    "        K = self.W_k(K).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2 )\n",
    "        V = self.W_v(V).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2 )\n",
    "        \n",
    "        weighted_values, attention = self.scale_dot_product(Q, K, V, mask)\n",
    "        weighted_values = weighted_values.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads*self.d_k)\n",
    "        weighted_values = self.W_o(weighted_values)\n",
    "        \n",
    "        return weighted_values, attention\n",
    "        \n",
    "        \n",
    "    def scale_dot_product(self, Q, K, V, mask = None):\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        attention = F.softmax(scores, dim = -1)\n",
    "        weighted_values = torch.matmul(attention, V)\n",
    "        \n",
    "        return weighted_values, attention\n",
    "        \n",
    "\n",
    "class PositionFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear2(F.relu(self.linear1(x)))\n",
    "    \n",
    "class EncoderSubLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = PositionFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.droupout1 = nn.Dropout(dropout)\n",
    "        self.droupout2 = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask = None):\n",
    "        attention_score, _ = self.self_attn(x, x, x, mask)\n",
    "        x = x + self.droupout1(attention_score)\n",
    "        x = self.norm1(x)\n",
    "        x = x + self.droupout2(self.ffn(x))\n",
    "        return self.norm2(x)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([EncoderSubLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "    def forward(self, x, mask=None):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "class DecoderSubLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, encoder_output, target_mask=None, encoder_mask=None):\n",
    "        attention_score, _ = self.self_attn(x, x, x, target_mask)\n",
    "        x = x + self.dropout1(attention_score)\n",
    "        x = self.norm1(x)\n",
    "        \n",
    "        encoder_attn, _ = self.cross_attn(x, encoder_output, encoder_output, encoder_mask)\n",
    "        x = x + self.dropout2(encoder_attn)\n",
    "        x = self.norm2(x)\n",
    "        \n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = x + self.dropout3(ff_output)\n",
    "        return self.norm3(x)\n",
    "        \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([DecoderSubLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, x, encoder_output, target_mask, encoder_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output, target_mask, encoder_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61070162",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, num_layers,\n",
    "                 input_vocab_size, target_vocab_size, \n",
    "                 max_len=MAX_SEQ_LEN, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.encoder_embedding = nn.Embedding(input_vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_embedding = PositionalEmbedding(d_model, max_len)\n",
    "        self.encoder = Encoder(d_model, num_heads, d_ff, num_layers, dropout)\n",
    "        self.decoder = Decoder(d_model, num_heads, d_ff, num_layers, dropout)\n",
    "        self.output_layer = nn.Linear(d_model, target_vocab_size)\n",
    "        \n",
    "    def forward(self, source, target):\n",
    "        # Encoder mask\n",
    "        source_mask, target_mask = self.mask(source, target)\n",
    "        # Embedding and positional Encoding\n",
    "        source = self.encoder_embedding(source) * math.sqrt(self.encoder_embedding.embedding_dim)\n",
    "        source = self.pos_embedding(source)\n",
    "        # Encoder\n",
    "        encoder_output = self.encoder(source, source_mask)\n",
    "        \n",
    "        # Decoder embedding and postional encoding\n",
    "        target = self.decoder_embedding(target) * math.sqrt(self.decoder_embedding.embedding_dim)\n",
    "        target = self.pos_embedding(target)\n",
    "        # Decoder\n",
    "        output = self.decoder(target, encoder_output, target_mask, source_mask)\n",
    "        \n",
    "        return self.output_layer(output)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def mask(self, source, target):\n",
    "        source_mask = (source != 0).unsqueeze(1).unsqueeze(2)\n",
    "        target_mask = (target != 0).unsqueeze(1).unsqueeze(2)\n",
    "        size = target.size(1)\n",
    "        no_mask = torch.tril(torch.ones((1, size, size), device=device)).bool()\n",
    "        target_mask = target_mask & no_mask\n",
    "        return source_mask, target_mask\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da6b2d4",
   "metadata": {},
   "source": [
    "#### Simple test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40581d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len_source = 10\n",
    "seq_len_target = 10\n",
    "batch_size = 2\n",
    "input_vocab_size = 50\n",
    "target_vocab_size = 50\n",
    "\n",
    "source = torch.randint(1, input_vocab_size, (batch_size, seq_len_source))\n",
    "target = torch.randint(1, target_vocab_size, (batch_size, seq_len_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7cf689",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 512\n",
    "num_heads = 8\n",
    "d_ff = 2048\n",
    "num_layers = 6\n",
    "\n",
    "model = Transformer(d_model, num_heads, d_ff, num_layers,\n",
    "                  input_vocab_size, target_vocab_size, \n",
    "                  max_len=MAX_SEQ_LEN, dropout=0.1)\n",
    "\n",
    "model = model.to(device)\n",
    "source = source.to(device)\n",
    "target = target.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4618560e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(source, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0bc69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected output shape -> [batch, seq_len_target, target_vocab_size] i.e. [2, 10, 50]\n",
    "print(f'ouput.shape {output.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fc9c4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a3b18d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50740746",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c0db72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ceefe95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e10a50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffb7af9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6321db74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccce7864",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
