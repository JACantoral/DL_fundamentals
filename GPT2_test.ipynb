{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cddd2f78-664d-42a7-8ecc-c92cc94052ea",
   "metadata": {},
   "source": [
    "## GPT2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f3a311-bbfe-478b-87e6-8d0c506d2be1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd86d216-3ab8-4ba5-a773-0d5d9f9d94cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1376ca9-ee4b-4af5-a347-cc4906e7cf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let us start by creating a class for configuration\n",
    "class Config:\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    def __init__(self, vocab_size = 50257, max_seq_length = 128, embed_size = 768, \n",
    "                 num_layers = 12, num_heads = 12, dropout = 0.1):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.embed_size = embed_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cf1119a2-f6ec-4d66-94db-15da3a9baf9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    '''\n",
    "    this is the mos important part, the heart of the Transformer\n",
    "    In this implementation of the Transformer, the embedding dimensiionality is kept through the entire\n",
    "    layers of the architecture. Recall that embed_size is a multiple of num_heads\n",
    "    '''\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.embed_size % config.num_heads == 0, 'Embed size not a multiple of num heads'\n",
    "        self.num_heads = config.num_heads\n",
    "        self.head_dim = config.embed_size // config.num_heads\n",
    "        # Get the linear transformations queries, keys, valies, and output FFN\n",
    "        self.W_q = nn.Linear(config.embed_size, config.embed_size)\n",
    "        self.W_k = nn.Linear(config.embed_size, config.embed_size)\n",
    "        self.W_v = nn.Linear(config.embed_size, config.embed_size)\n",
    "        self.output = nn.Linear(config.embed_size, config.embed_size)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        # lower triangular mask for causal attention\n",
    "        self.register_buffer(\n",
    "            'mask',\n",
    "            torch.tril(torch.ones(config.max_seq_length, config.max_seq_length)\n",
    "                      ).view(-1, 1, config.max_seq_length, config.max_seq_length))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch, seq_length, embed_dim = x.size() # e.g. 16, 128, 768\n",
    "        # recall multihead attention embed_dim % num_head = 0\n",
    "        Q = self.W_q(x).view(batch, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = self.W_k(x).view(batch, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = self.W_v(x).view(batch, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        # After transpose V.shape is (batch, self.num_heads, seq_lengths, head_dim)\n",
    "        # causal (masked) dot products\n",
    "        attn = (Q@K.transpose(-2, -1)) / (self.head_dim**5)  \n",
    "                                      # Q, K.shape is (batch, num_heads, seq_lengths, head_dim)\n",
    "                                      # K.transpose(-2, -1).shape is batch, num_heads, head_dim, seq_length)\n",
    "        attn = attn.masked_fill(self.mask[:, :, :seq_length, :seq_length] == 0, float('-inf'))\n",
    "        attn = F.softmax(attn, dim = -1)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "\n",
    "        scores = attn @ V #attn.shape is (batch, num_heads, seq_lengths, seq_lengths)\n",
    "                        #scores will be (batch, num_heads, seq_lengths, head_dim)\n",
    "        \n",
    "        scores = scores.transpose(1, 2).contiguous().view(batch, seq_length, embed_dim)\n",
    "        scores = self.output(scores)\n",
    "        return self.dropout(scores)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5f181cd7-c778-447e-a63a-ee36b4564007",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    '''\n",
    "    MLP for transformer block\n",
    "    '''\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(config.embed_size, 4 * config.embed_size)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.fc2 = nn.Linear(4*config.embed_size, config.embed_size)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc2(self.gelu(self.fc1(x)))\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c7514a48-f391-42a0-9b4f-2f0073f73c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    '''\n",
    "    Put the transformer together\n",
    "    '''\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(config.embed_size)\n",
    "        self.attention = SelfAttention(config)\n",
    "        self.norm2 = nn.LayerNorm(config.embed_size)\n",
    "        self.mlp = FFN(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attention(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4267bdd8-1aaf-446d-b5c4-5ea9cb31bb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2(nn.Module):\n",
    "    '''\n",
    "    Building the entire GPT2 by combining the other modules\n",
    "    '''\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.token_embed = nn.Embedding(config.vocab_size, config.embed_size)\n",
    "        self.pos_embed = nn.Embedding(config.max_seq_length, config.embed_size)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.transformers = nn.Sequential(*[Transformer(config) for _ in range(config.num_layers)])\n",
    "        self.norm1 = nn.LayerNorm(config.embed_size)\n",
    "\n",
    "    def forward(self, input_tokens):\n",
    "        batch, seq_length = input_tokens.size()\n",
    "        pos = torch.arange(0, seq_length, dtype= torch.long, device=input_tokens.device).unsqueeze(0)\n",
    "        x = self.token_embed(input_tokens) +self.pos_embed(pos) #x shape will be (batch, seq_length, emb)\n",
    "        x = self.dropout(x)\n",
    "        x = self.transformers(x)\n",
    "        x = self.norm1(x)\n",
    "        return x @ self.token_embed.weight.t()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5551cd06-02c6-44a7-82fe-afdde923aa0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "# Download Don Quijote \n",
    "url = \"https://www.gutenberg.org/cache/epub/2000/pg2000.txt\"\n",
    "response = requests.get(url)\n",
    "text = response.text\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d4477ae0-f108-4ca0-bfb7-7a25071c7625",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c5dda297-582b-4d89-b2a4-0122e853f50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (860018 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "tokeniser = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "tokeniser.pad_token = tokeniser.eos_token\n",
    "\n",
    "tokens = tokeniser.encode(text)\n",
    "data = torch.tensor(tokens, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "be91e331-ac1f-4356-bff8-76dec2849a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LENTGH = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "65f966cf-89cc-480d-9753-e9496748d71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a class for Don Quijote text\n",
    "class Quijote(Dataset):\n",
    "    '''\n",
    "    This class is create training samples of length SEQ_LENGTH    \n",
    "    '''\n",
    "    def __init__(self, data, seq_length):\n",
    "        self.text = data\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text) - self.seq_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.text[idx: idx + self.seq_length]\n",
    "        y = self.text[idx + 1: idx + self.seq_length + 1]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3e42b835-d2dc-49b4-ba7d-61b887dfd28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "quijote = Quijote(data, SEQ_LENTGH)\n",
    "quijote_loader = DataLoader(quijote, batch_size=16, shuffle= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "65ccfda5-a5ac-48ec-9e0a-e50ac1894f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(\n",
    "    vocab_size = tokeniser.vocab_size,\n",
    "    max_seq_length = SEQ_LENTGH, \n",
    "    embed_size = 128, \n",
    "    num_layers = 4,\n",
    "    num_heads = 4,\n",
    "    dropout = 0.1\n",
    ")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GPT2(config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d720a055-cc33-4b29-8ddc-2d3772cc45fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser = optim.Adam(model.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9de2d75a-f9c6-4625-af75-eda44f136063",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimiser, epochs = 10):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        for i, (x, y) in enumerate(loader):\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            optimiser.zero_grad()\n",
    "            scores = model(x) #shape is (bminibatch, seq_lenght, vocab_size)\n",
    "            loss = F.cross_entropy(scores.view(-1, scores.size(-1)), y.view(-1)) #y is shape (B, T)\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            if i%500 == 0:\n",
    "                print(f'epoch: {epoch+1}, step: {i}, loss:{loss.item():.4f}')\n",
    "\n",
    "        epoch_loss = total_loss / len(loader)\n",
    "        print(f'epoch: {epoch + 1}. Loss: {epoch_loss:.4f}')\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3d11689d-bf58-4491-b172-29fffa8cb40c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, step: 0, loss:82.6065\n",
      "epoch: 1, step: 500, loss:8.6013\n",
      "epoch: 1, step: 1000, loss:6.4399\n",
      "epoch: 1, step: 1500, loss:5.8266\n",
      "epoch: 1, step: 2000, loss:5.6091\n",
      "epoch: 1, step: 2500, loss:5.2683\n",
      "epoch: 1, step: 3000, loss:4.8214\n",
      "epoch: 1, step: 3500, loss:4.9609\n",
      "epoch: 1, step: 4000, loss:4.6638\n",
      "epoch: 1, step: 4500, loss:4.7116\n",
      "epoch: 1, step: 5000, loss:4.6064\n",
      "epoch: 1, step: 5500, loss:4.3360\n",
      "epoch: 1, step: 6000, loss:4.2461\n",
      "epoch: 1, step: 6500, loss:4.2833\n",
      "epoch: 1, step: 7000, loss:4.2021\n",
      "epoch: 1, step: 7500, loss:4.1057\n",
      "epoch: 1, step: 8000, loss:4.0539\n",
      "epoch: 1, step: 8500, loss:4.0348\n",
      "epoch: 1, step: 9000, loss:4.0038\n",
      "epoch: 1, step: 9500, loss:3.8860\n",
      "epoch: 1, step: 10000, loss:3.8837\n",
      "epoch: 1, step: 10500, loss:3.8654\n",
      "epoch: 1, step: 11000, loss:3.9115\n",
      "epoch: 1, step: 11500, loss:3.6369\n",
      "epoch: 1, step: 12000, loss:3.8873\n",
      "epoch: 1, step: 12500, loss:3.8105\n",
      "epoch: 1, step: 13000, loss:3.7647\n",
      "epoch: 1, step: 13500, loss:3.8290\n",
      "epoch: 1, step: 14000, loss:3.6231\n",
      "epoch: 1, step: 14500, loss:3.6282\n",
      "epoch: 1, step: 15000, loss:3.7732\n",
      "epoch: 1, step: 15500, loss:3.7325\n",
      "epoch: 1, step: 16000, loss:3.6438\n",
      "epoch: 1, step: 16500, loss:3.6889\n",
      "epoch: 1, step: 17000, loss:3.6259\n",
      "epoch: 1, step: 17500, loss:3.6241\n",
      "epoch: 1, step: 18000, loss:3.7015\n",
      "epoch: 1, step: 18500, loss:3.6057\n",
      "epoch: 1, step: 19000, loss:3.5282\n",
      "epoch: 1, step: 19500, loss:3.6168\n",
      "epoch: 1, step: 20000, loss:3.4114\n",
      "epoch: 1, step: 20500, loss:3.6342\n",
      "epoch: 1, step: 21000, loss:3.6320\n",
      "epoch: 1, step: 21500, loss:3.5784\n",
      "epoch: 1, step: 22000, loss:3.4719\n",
      "epoch: 1, step: 22500, loss:3.7517\n",
      "epoch: 1, step: 23000, loss:3.4605\n",
      "epoch: 1, step: 23500, loss:3.5965\n",
      "epoch: 1, step: 24000, loss:3.5130\n",
      "epoch: 1, step: 24500, loss:3.4313\n",
      "epoch: 1, step: 25000, loss:3.4078\n",
      "epoch: 1, step: 25500, loss:3.4461\n",
      "epoch: 1, step: 26000, loss:3.4446\n",
      "epoch: 1, step: 26500, loss:3.3891\n",
      "epoch: 1, step: 27000, loss:3.4854\n",
      "epoch: 1, step: 27500, loss:3.3284\n",
      "epoch: 1, step: 28000, loss:3.5234\n",
      "epoch: 1, step: 28500, loss:3.6207\n",
      "epoch: 1, step: 29000, loss:3.3957\n",
      "epoch: 1, step: 29500, loss:3.3296\n",
      "epoch: 1, step: 30000, loss:3.5384\n",
      "epoch: 1, step: 30500, loss:3.3763\n",
      "epoch: 1, step: 31000, loss:3.4958\n",
      "epoch: 1, step: 31500, loss:3.3963\n",
      "epoch: 1, step: 32000, loss:3.5309\n",
      "epoch: 1, step: 32500, loss:3.4979\n",
      "epoch: 1, step: 33000, loss:3.3965\n",
      "epoch: 1, step: 33500, loss:3.3483\n",
      "epoch: 1, step: 34000, loss:3.3537\n",
      "epoch: 1, step: 34500, loss:3.2987\n",
      "epoch: 1, step: 35000, loss:3.5109\n",
      "epoch: 1, step: 35500, loss:3.3526\n",
      "epoch: 1, step: 36000, loss:3.2972\n",
      "epoch: 1, step: 36500, loss:3.1740\n",
      "epoch: 1, step: 37000, loss:3.2247\n",
      "epoch: 1, step: 37500, loss:3.2233\n",
      "epoch: 1, step: 38000, loss:3.3767\n",
      "epoch: 1, step: 38500, loss:3.2338\n",
      "epoch: 1, step: 39000, loss:3.3530\n",
      "epoch: 1, step: 39500, loss:3.2990\n",
      "epoch: 1, step: 40000, loss:3.3018\n",
      "epoch: 1, step: 40500, loss:3.3946\n",
      "epoch: 1, step: 41000, loss:3.3191\n",
      "epoch: 1, step: 41500, loss:3.3124\n",
      "epoch: 1, step: 42000, loss:3.1841\n",
      "epoch: 1, step: 42500, loss:3.3677\n",
      "epoch: 1, step: 43000, loss:3.2895\n",
      "epoch: 1, step: 43500, loss:3.3286\n",
      "epoch: 1, step: 44000, loss:3.2897\n",
      "epoch: 1, step: 44500, loss:3.3175\n",
      "epoch: 1, step: 45000, loss:3.2783\n",
      "epoch: 1, step: 45500, loss:3.2346\n",
      "epoch: 1, step: 46000, loss:3.2371\n",
      "epoch: 1, step: 46500, loss:3.1193\n",
      "epoch: 1, step: 47000, loss:3.1527\n",
      "epoch: 1, step: 47500, loss:3.0995\n",
      "epoch: 1, step: 48000, loss:3.1905\n",
      "epoch: 1, step: 48500, loss:3.2453\n",
      "epoch: 1, step: 49000, loss:3.2341\n",
      "epoch: 1, step: 49500, loss:3.2174\n",
      "epoch: 1, step: 50000, loss:3.2319\n",
      "epoch: 1, step: 50500, loss:3.2384\n",
      "epoch: 1, step: 51000, loss:3.2138\n",
      "epoch: 1, step: 51500, loss:3.2867\n",
      "epoch: 1, step: 52000, loss:3.0896\n",
      "epoch: 1, step: 52500, loss:3.2694\n",
      "epoch: 1, step: 53000, loss:3.1216\n",
      "epoch: 1, step: 53500, loss:3.1930\n",
      "epoch: 1. Loss: 3.8046\n",
      "epoch: 2, step: 0, loss:3.3289\n",
      "epoch: 2, step: 500, loss:3.1411\n",
      "epoch: 2, step: 1000, loss:3.1495\n",
      "epoch: 2, step: 1500, loss:3.0881\n",
      "epoch: 2, step: 2000, loss:3.0872\n",
      "epoch: 2, step: 2500, loss:3.2752\n",
      "epoch: 2, step: 3000, loss:3.3274\n",
      "epoch: 2, step: 3500, loss:3.0870\n",
      "epoch: 2, step: 4000, loss:3.1322\n",
      "epoch: 2, step: 4500, loss:2.9542\n",
      "epoch: 2, step: 5000, loss:3.1617\n",
      "epoch: 2, step: 5500, loss:3.1679\n",
      "epoch: 2, step: 6000, loss:3.3792\n",
      "epoch: 2, step: 6500, loss:3.1856\n",
      "epoch: 2, step: 7000, loss:3.1012\n",
      "epoch: 2, step: 7500, loss:3.1440\n",
      "epoch: 2, step: 8000, loss:3.1948\n",
      "epoch: 2, step: 8500, loss:2.9555\n",
      "epoch: 2, step: 9000, loss:2.9395\n",
      "epoch: 2, step: 9500, loss:3.1402\n",
      "epoch: 2, step: 10000, loss:3.1669\n",
      "epoch: 2, step: 10500, loss:3.1886\n",
      "epoch: 2, step: 11000, loss:3.1314\n",
      "epoch: 2, step: 11500, loss:3.1006\n",
      "epoch: 2, step: 12000, loss:3.1861\n",
      "epoch: 2, step: 12500, loss:3.1061\n",
      "epoch: 2, step: 13000, loss:3.1980\n",
      "epoch: 2, step: 13500, loss:3.2209\n",
      "epoch: 2, step: 14000, loss:3.1121\n",
      "epoch: 2, step: 14500, loss:3.1807\n",
      "epoch: 2, step: 15000, loss:3.1123\n",
      "epoch: 2, step: 15500, loss:3.1567\n",
      "epoch: 2, step: 16000, loss:3.0688\n",
      "epoch: 2, step: 16500, loss:3.1479\n",
      "epoch: 2, step: 17000, loss:3.0122\n",
      "epoch: 2, step: 17500, loss:3.2013\n",
      "epoch: 2, step: 18000, loss:3.1134\n",
      "epoch: 2, step: 18500, loss:3.1714\n",
      "epoch: 2, step: 19000, loss:3.1505\n",
      "epoch: 2, step: 19500, loss:3.0209\n",
      "epoch: 2, step: 20000, loss:3.0622\n",
      "epoch: 2, step: 20500, loss:3.0013\n",
      "epoch: 2, step: 21000, loss:3.1367\n",
      "epoch: 2, step: 21500, loss:3.1750\n",
      "epoch: 2, step: 22000, loss:3.0900\n",
      "epoch: 2, step: 22500, loss:3.0163\n",
      "epoch: 2, step: 23000, loss:3.0039\n",
      "epoch: 2, step: 23500, loss:3.1113\n",
      "epoch: 2, step: 24000, loss:3.0011\n",
      "epoch: 2, step: 24500, loss:2.8171\n",
      "epoch: 2, step: 25000, loss:3.1280\n",
      "epoch: 2, step: 25500, loss:3.0693\n",
      "epoch: 2, step: 26000, loss:3.1316\n",
      "epoch: 2, step: 26500, loss:3.0588\n",
      "epoch: 2, step: 27000, loss:3.0069\n",
      "epoch: 2, step: 27500, loss:2.8984\n",
      "epoch: 2, step: 28000, loss:3.0327\n",
      "epoch: 2, step: 28500, loss:3.0441\n",
      "epoch: 2, step: 29000, loss:3.0228\n",
      "epoch: 2, step: 29500, loss:3.0538\n",
      "epoch: 2, step: 30000, loss:3.0994\n",
      "epoch: 2, step: 30500, loss:3.1789\n",
      "epoch: 2, step: 31000, loss:3.0459\n",
      "epoch: 2, step: 31500, loss:3.1284\n",
      "epoch: 2, step: 32000, loss:3.0852\n",
      "epoch: 2, step: 32500, loss:3.1436\n",
      "epoch: 2, step: 33000, loss:2.9751\n",
      "epoch: 2, step: 33500, loss:2.9081\n",
      "epoch: 2, step: 34000, loss:3.0887\n",
      "epoch: 2, step: 34500, loss:3.0504\n",
      "epoch: 2, step: 35000, loss:3.0213\n",
      "epoch: 2, step: 35500, loss:3.0489\n",
      "epoch: 2, step: 36000, loss:3.0206\n",
      "epoch: 2, step: 36500, loss:2.9740\n",
      "epoch: 2, step: 37000, loss:3.0895\n",
      "epoch: 2, step: 37500, loss:2.9876\n",
      "epoch: 2, step: 38000, loss:2.9797\n",
      "epoch: 2, step: 38500, loss:3.0225\n",
      "epoch: 2, step: 39000, loss:2.9289\n",
      "epoch: 2, step: 39500, loss:3.0895\n",
      "epoch: 2, step: 40000, loss:3.0893\n",
      "epoch: 2, step: 40500, loss:2.9600\n",
      "epoch: 2, step: 41000, loss:3.2129\n",
      "epoch: 2, step: 41500, loss:2.8546\n",
      "epoch: 2, step: 42000, loss:3.0094\n",
      "epoch: 2, step: 42500, loss:3.0082\n",
      "epoch: 2, step: 43000, loss:2.9305\n",
      "epoch: 2, step: 43500, loss:3.0827\n",
      "epoch: 2, step: 44000, loss:2.9789\n",
      "epoch: 2, step: 44500, loss:3.1112\n",
      "epoch: 2, step: 45000, loss:3.0496\n",
      "epoch: 2, step: 45500, loss:3.0191\n",
      "epoch: 2, step: 46000, loss:2.8513\n",
      "epoch: 2, step: 46500, loss:2.9073\n",
      "epoch: 2, step: 47000, loss:3.0443\n",
      "epoch: 2, step: 47500, loss:3.0278\n",
      "epoch: 2, step: 48000, loss:2.9954\n",
      "epoch: 2, step: 48500, loss:3.0600\n",
      "epoch: 2, step: 49000, loss:3.0921\n",
      "epoch: 2, step: 49500, loss:3.0012\n",
      "epoch: 2, step: 50000, loss:3.1317\n",
      "epoch: 2, step: 50500, loss:2.9553\n",
      "epoch: 2, step: 51000, loss:2.8707\n",
      "epoch: 2, step: 51500, loss:3.0103\n",
      "epoch: 2, step: 52000, loss:2.8398\n",
      "epoch: 2, step: 52500, loss:3.1034\n",
      "epoch: 2, step: 53000, loss:2.9276\n",
      "epoch: 2, step: 53500, loss:3.0004\n",
      "epoch: 2. Loss: 3.0800\n",
      "epoch: 3, step: 0, loss:3.0844\n",
      "epoch: 3, step: 500, loss:3.0511\n",
      "epoch: 3, step: 1000, loss:3.0814\n",
      "epoch: 3, step: 1500, loss:2.9701\n",
      "epoch: 3, step: 2000, loss:2.9133\n",
      "epoch: 3, step: 2500, loss:2.8677\n",
      "epoch: 3, step: 3000, loss:2.9908\n",
      "epoch: 3, step: 3500, loss:2.9729\n",
      "epoch: 3, step: 4000, loss:2.9683\n",
      "epoch: 3, step: 4500, loss:3.0056\n",
      "epoch: 3, step: 5000, loss:2.9093\n",
      "epoch: 3, step: 5500, loss:3.0756\n",
      "epoch: 3, step: 6000, loss:2.9949\n",
      "epoch: 3, step: 6500, loss:2.9561\n",
      "epoch: 3, step: 7000, loss:3.0600\n",
      "epoch: 3, step: 7500, loss:3.0316\n",
      "epoch: 3, step: 8000, loss:2.8940\n",
      "epoch: 3, step: 8500, loss:2.9704\n",
      "epoch: 3, step: 9000, loss:3.1742\n",
      "epoch: 3, step: 9500, loss:2.9561\n",
      "epoch: 3, step: 10000, loss:2.9917\n",
      "epoch: 3, step: 10500, loss:3.0841\n",
      "epoch: 3, step: 11000, loss:2.9459\n",
      "epoch: 3, step: 11500, loss:2.7500\n",
      "epoch: 3, step: 12000, loss:2.9174\n",
      "epoch: 3, step: 12500, loss:2.8091\n",
      "epoch: 3, step: 13000, loss:2.9190\n",
      "epoch: 3, step: 13500, loss:2.8921\n",
      "epoch: 3, step: 14000, loss:2.9351\n",
      "epoch: 3, step: 14500, loss:2.9178\n",
      "epoch: 3, step: 15000, loss:2.8561\n",
      "epoch: 3, step: 15500, loss:2.8190\n",
      "epoch: 3, step: 16000, loss:3.0710\n",
      "epoch: 3, step: 16500, loss:2.8479\n",
      "epoch: 3, step: 17000, loss:2.9390\n",
      "epoch: 3, step: 17500, loss:2.9579\n",
      "epoch: 3, step: 18000, loss:2.8232\n",
      "epoch: 3, step: 18500, loss:2.8523\n",
      "epoch: 3, step: 19000, loss:2.9057\n",
      "epoch: 3, step: 19500, loss:2.9868\n",
      "epoch: 3, step: 20000, loss:2.9215\n",
      "epoch: 3, step: 20500, loss:2.9111\n",
      "epoch: 3, step: 21000, loss:2.9609\n",
      "epoch: 3, step: 21500, loss:2.7654\n",
      "epoch: 3, step: 22000, loss:2.8582\n",
      "epoch: 3, step: 22500, loss:3.0472\n",
      "epoch: 3, step: 23000, loss:2.8914\n",
      "epoch: 3, step: 23500, loss:2.9893\n",
      "epoch: 3, step: 24000, loss:2.9999\n",
      "epoch: 3, step: 24500, loss:2.8579\n",
      "epoch: 3, step: 25000, loss:2.9518\n",
      "epoch: 3, step: 25500, loss:2.8275\n",
      "epoch: 3, step: 26000, loss:2.7951\n",
      "epoch: 3, step: 26500, loss:2.7681\n",
      "epoch: 3, step: 27000, loss:2.6789\n",
      "epoch: 3, step: 27500, loss:3.0268\n",
      "epoch: 3, step: 28000, loss:3.0707\n",
      "epoch: 3, step: 28500, loss:2.8591\n",
      "epoch: 3, step: 29000, loss:3.0250\n",
      "epoch: 3, step: 29500, loss:2.9962\n",
      "epoch: 3, step: 30000, loss:2.8804\n",
      "epoch: 3, step: 30500, loss:2.9688\n",
      "epoch: 3, step: 31000, loss:2.8739\n",
      "epoch: 3, step: 31500, loss:2.8394\n",
      "epoch: 3, step: 32000, loss:3.0041\n",
      "epoch: 3, step: 32500, loss:3.1285\n",
      "epoch: 3, step: 33000, loss:2.7889\n",
      "epoch: 3, step: 33500, loss:2.9512\n",
      "epoch: 3, step: 34000, loss:2.9218\n",
      "epoch: 3, step: 34500, loss:2.9304\n",
      "epoch: 3, step: 35000, loss:2.9479\n",
      "epoch: 3, step: 35500, loss:2.9792\n",
      "epoch: 3, step: 36000, loss:2.9502\n",
      "epoch: 3, step: 36500, loss:3.0580\n",
      "epoch: 3, step: 37000, loss:2.8711\n",
      "epoch: 3, step: 37500, loss:2.9036\n",
      "epoch: 3, step: 38000, loss:2.9128\n",
      "epoch: 3, step: 38500, loss:2.9575\n",
      "epoch: 3, step: 39000, loss:2.8167\n",
      "epoch: 3, step: 39500, loss:2.9160\n",
      "epoch: 3, step: 40000, loss:2.8762\n",
      "epoch: 3, step: 40500, loss:2.7778\n",
      "epoch: 3, step: 41000, loss:2.8254\n",
      "epoch: 3, step: 41500, loss:2.8168\n",
      "epoch: 3, step: 42000, loss:2.9164\n",
      "epoch: 3, step: 42500, loss:2.8691\n",
      "epoch: 3, step: 43000, loss:2.8574\n",
      "epoch: 3, step: 43500, loss:2.9168\n",
      "epoch: 3, step: 44000, loss:2.9271\n",
      "epoch: 3, step: 44500, loss:2.8834\n",
      "epoch: 3, step: 45000, loss:2.9453\n",
      "epoch: 3, step: 45500, loss:2.8642\n",
      "epoch: 3, step: 46000, loss:3.0367\n",
      "epoch: 3, step: 46500, loss:2.8717\n",
      "epoch: 3, step: 47000, loss:2.8243\n",
      "epoch: 3, step: 47500, loss:2.8471\n",
      "epoch: 3, step: 48000, loss:2.8789\n",
      "epoch: 3, step: 48500, loss:2.8516\n",
      "epoch: 3, step: 49000, loss:2.8889\n",
      "epoch: 3, step: 49500, loss:2.8650\n",
      "epoch: 3, step: 50000, loss:2.9107\n",
      "epoch: 3, step: 50500, loss:2.9555\n",
      "epoch: 3, step: 51000, loss:2.8608\n",
      "epoch: 3, step: 51500, loss:2.7573\n",
      "epoch: 3, step: 52000, loss:2.8384\n",
      "epoch: 3, step: 52500, loss:2.8378\n",
      "epoch: 3, step: 53000, loss:2.8187\n",
      "epoch: 3, step: 53500, loss:2.9520\n",
      "epoch: 3. Loss: 2.9327\n"
     ]
    }
   ],
   "source": [
    "train(model, quijote_loader, optimiser, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8053e7ec-476a-4867-92fd-567576dd00cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3118,  1556,   463,  3014,    68,   390,  6253,  4533,   836,  2264,\n",
      "          2926,  1258,    13,   201,   198,   201,   198,   960,   412,  4169,\n",
      "           269,   723,    81, 20954,   390, 18912, 21162,  1288,   331,  2207,\n",
      "           415, 28213, 37911,  8591,  1667,  3318, 28213,    11,   390,  8591,\n",
      "           339,   445, 13533,   551,   555,    64,   201,   198,    69, 30997,\n",
      "           851,  4188,  1288,  1090,    64, 23694,  3031,    72]],\n",
      "       device='cuda:0')\n",
      "Un estudiante de doctorado don Quijote.\n",
      "\n",
      "— Este cualrés de cuán el y encantoso fue la mar undoso, de la heredonda en una\n",
      "frica —que el cura Luna respondi\n"
     ]
    }
   ],
   "source": [
    "def sample(model, device, tokenizer, prompt, length=50, temperature = 1.0):\n",
    "    model.eval()\n",
    "    tokens = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "\n",
    "    for _ in range(length):\n",
    "        tokens_cond = tokens[:, -SEQ_LENTGH:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(tokens_cond)\n",
    "        next_token_logits = logits[:, -1, :] / temperature\n",
    "        next_token = torch.multinomial(F.softmax(next_token_logits, dim = -1), num_samples = 1)\n",
    "        tokens = torch.cat([tokens, next_token], dim = 1)\n",
    "        \n",
    "    print(tokens)\n",
    "    return tokenizer.decode(tokens[0])\n",
    "\n",
    "# Example usage\n",
    "print(sample(model, device, tokeniser, prompt=\"Un estudiante de doctorado\", length=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af30337d-7057-4d59-a1db-1e83b9aed072",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9849eb8-c498-4df9-a7c1-421e517f78f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f9b4ba-6349-409b-8b98-202260252111",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3c176c-6648-464c-8677-87d9ef4319e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d05b1e-3045-4610-853a-3e8483de2652",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64f1067-73ca-4305-a70a-131a4a3e0768",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421b6762-6fa3-47da-9c50-3cd08459b22d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd8cc9f-2dcb-4a63-b243-df3eb58ce35f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd50023-63c7-4b45-84ba-f5c0061ad4ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de9ef10-8073-4cdb-9611-e4b739d75746",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b295ce-8bb8-493e-8a24-16e46b8efff4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e771833-14bb-4113-8ec1-cb70c99e54bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
